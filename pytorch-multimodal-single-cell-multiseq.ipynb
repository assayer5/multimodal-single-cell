{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:06.352001Z",
     "iopub.status.busy": "2022-11-17T05:24:06.351612Z",
     "iopub.status.idle": "2022-11-17T05:24:31.673292Z",
     "shell.execute_reply": "2022-11-17T05:24:31.671817Z",
     "shell.execute_reply.started": "2022-11-17T05:24:06.351969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tables in /opt/conda/lib/python3.7/site-packages (3.7.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tables) (21.3)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.7/site-packages (from tables) (1.21.6)\n",
      "Requirement already satisfied: numexpr>=2.6.2 in /opt/conda/lib/python3.7/site-packages (from tables) (2.8.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->tables) (3.0.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting hdf5plugin\n",
      "  Downloading hdf5plugin-3.3.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from hdf5plugin) (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from h5py->hdf5plugin) (1.21.6)\n",
      "Installing collected packages: hdf5plugin\n",
      "Successfully installed hdf5plugin-3.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m/kaggle/input/open-problems-multimodal/sample_submission.csv\n",
      "/kaggle/input/open-problems-multimodal/train_cite_targets.h5\n",
      "/kaggle/input/open-problems-multimodal/metadata_cite_day_2_donor_27678.csv\n",
      "/kaggle/input/open-problems-multimodal/test_multi_inputs.h5\n",
      "/kaggle/input/open-problems-multimodal/evaluation_ids.csv\n",
      "/kaggle/input/open-problems-multimodal/train_cite_inputs.h5\n",
      "/kaggle/input/open-problems-multimodal/train_multi_targets.h5\n",
      "/kaggle/input/open-problems-multimodal/train_multi_inputs.h5\n",
      "/kaggle/input/open-problems-multimodal/metadata.csv\n",
      "/kaggle/input/open-problems-multimodal/test_cite_inputs_day_2_donor_27678.h5\n",
      "/kaggle/input/open-problems-multimodal/test_cite_inputs.h5\n"
     ]
    }
   ],
   "source": [
    "# need 'tables' package to read h5 files\n",
    "#!conda install -c ska tables\n",
    "!pip install tables\n",
    "!pip install hdf5plugin\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:31.675946Z",
     "iopub.status.busy": "2022-11-17T05:24:31.675541Z",
     "iopub.status.idle": "2022-11-17T05:24:33.552309Z",
     "shell.execute_reply": "2022-11-17T05:24:33.551107Z",
     "shell.execute_reply.started": "2022-11-17T05:24:31.675912Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import h5py, hdf5plugin\n",
    "\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:33.555479Z",
     "iopub.status.busy": "2022-11-17T05:24:33.554708Z",
     "iopub.status.idle": "2022-11-17T05:24:33.560613Z",
     "shell.execute_reply": "2022-11-17T05:24:33.559698Z",
     "shell.execute_reply.started": "2022-11-17T05:24:33.555428Z"
    }
   },
   "outputs": [],
   "source": [
    "# file paths\n",
    "dir = '../input/open-problems-multimodal/'\n",
    "\n",
    "metadata = dir + 'metadata.csv'\n",
    "train_multi_inputs = dir + 'train_multi_inputs.h5'\n",
    "train_multi_targets = dir + 'train_multi_targets.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:33.576419Z",
     "iopub.status.busy": "2022-11-17T05:24:33.576003Z",
     "iopub.status.idle": "2022-11-17T05:24:33.591199Z",
     "shell.execute_reply": "2022-11-17T05:24:33.590040Z",
     "shell.execute_reply.started": "2022-11-17T05:24:33.576383Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to get metadata for xfile samples\n",
    "# inputs: directory - file directory, metafile - metadata csv, xfile - input file name\n",
    "def getmeta(directory, metafile, xfile):\n",
    "    metadf = pd.read_csv(directory + metafile)\n",
    "    \n",
    "    # index by cell id\n",
    "    metadf.set_index('cell_id', drop=True, inplace=True)\n",
    "\n",
    "    # get cell ids of dataset\n",
    "    with h5py.File(directory + xfile, 'r') as f:\n",
    "        dataset = f[xfile[:-3]]\n",
    "        samples = dataset['axis1'][:].astype(str).tolist() # cell ids\n",
    "    \n",
    "    # retain metadata for xfile samples\n",
    "    metadf = metadf.loc[samples]\n",
    "    metadf.reset_index(inplace=True)\n",
    "\n",
    "    return metadf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:33.593562Z",
     "iopub.status.busy": "2022-11-17T05:24:33.592817Z",
     "iopub.status.idle": "2022-11-17T05:24:33.603276Z",
     "shell.execute_reply": "2022-11-17T05:24:33.602441Z",
     "shell.execute_reply.started": "2022-11-17T05:24:33.593525Z"
    }
   },
   "outputs": [],
   "source": [
    "# find max values of input and output for scaling data\n",
    "# if finding max of subset, provide indexes of subset\n",
    "# output: column maximums of inputs (array), column maximums of outputs (array)\n",
    "def getmax(directory, xfilename, yfilename, indexes=None):\n",
    "    chunks = []\n",
    "    maxvals = []\n",
    "    for file in [xfilename, yfilename]:\n",
    "        with h5py.File(directory + file, 'r') as f:\n",
    "            dataset = f[file[:-3]]\n",
    "            if indexes == None:\n",
    "#                 values = dataset['block0_values'][:]\n",
    "                datashape = dataset['block0_values'].shape\n",
    "                idxarray = np.arange(datashape[0])\n",
    "            else:\n",
    "#                 values = dataset['block0_values'][indexes]\n",
    "                idxarray = np.array(indexes)\n",
    "    \n",
    "            # read file in chunks to avoid memory issues\n",
    "            for chunkidxs in np.array_split(idxarray, 100):\n",
    "                # sparse matrix to save space\n",
    "                chunk = csr_matrix(dataset['block0_values'][chunkidxs]) \n",
    "                chunks.append(chunk)\n",
    "        # combine chunks    \n",
    "        values = vstack(chunks)\n",
    "        chunks = []\n",
    "\n",
    "        # store maximums    \n",
    "        maxvals.append(np.max(values, axis=0))\n",
    "        values = None\n",
    "        \n",
    "    # convert sparse to dense\n",
    "    inputmax = maxvals[0].todense()\n",
    "    outputmax = maxvals[1].todense()\n",
    "    \n",
    "    # convert matrix to flattened array\n",
    "    inputmax = np.asarray(inputmax).ravel()\n",
    "    outputmax = np.asarray(outputmax).ravel()\n",
    "    \n",
    "    # if max value is 0, replace with 1 to avoid zero division when scaling\n",
    "    inputmax = np.where(inputmax <= 0, 1, inputmax)\n",
    "    outputmax = np.where(outputmax <= 0, 1, outputmax)\n",
    "    \n",
    "    return inputmax, outputmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:33.605346Z",
     "iopub.status.busy": "2022-11-17T05:24:33.604798Z",
     "iopub.status.idle": "2022-11-17T05:24:33.620419Z",
     "shell.execute_reply": "2022-11-17T05:24:33.619304Z",
     "shell.execute_reply.started": "2022-11-17T05:24:33.605307Z"
    }
   },
   "outputs": [],
   "source": [
    "# create custom dataset, subclass pytorch Dataset\n",
    "# need as inputs: \n",
    "# directory - file directory, xfile - input file name, yfile - target file name, \n",
    "# metadata - sample metadata as dataframe, xmax - array of max values per column of input data (for scaling),\n",
    "# ymax - array of max values per column of output data (for scaling), indexes - indexes of training subset (if any)\n",
    "class MakeDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, directory, xfile, yfile, metadata, xmax, ymax, indexes=None):\n",
    "        # initialize data\n",
    "        self.dir = directory\n",
    "        self.xfile = xfile\n",
    "        self.yfile = yfile\n",
    "        self.metadata = metadata\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "        self.indexes = indexes\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # read input and target from file on disk\n",
    "        data = []\n",
    "        for file in [self.xfile, self.yfile]:\n",
    "            with h5py.File(self.dir + file, 'r') as f:\n",
    "                dataset = f[file[:-3]]\n",
    "                value = dataset['block0_values'][index]\n",
    "            # store value from file    \n",
    "            data.append(value)\n",
    "        \n",
    "        # get day of index from metadata\n",
    "        day = self.metadata.loc[index, 'day']\n",
    "        \n",
    "        # get max day for scaling\n",
    "        if self.indexes == None:\n",
    "            maxday = self.metadata.loc[:, 'day'].max()\n",
    "        else:\n",
    "            maxday = self.metadata.loc[self.indexes, 'day'].max()\n",
    "        \n",
    "        # add day as feature of xdata, scale\n",
    "        xdata = np.append(data[0], day) / np.append(self.xmax, maxday)\n",
    "        ydata = data[1] / self.ymax\n",
    "        # replace nan with 0, convert to float tensors\n",
    "        xdata = torch.from_numpy(np.nan_to_num(xdata)).float()\n",
    "        ydata = torch.from_numpy(np.nan_to_num(ydata)).float()\n",
    "        \n",
    "        return xdata, ydata    \n",
    "    \n",
    "    def __len__(self):\n",
    "        # get length of dataset\n",
    "        return self.metadata.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:33.622575Z",
     "iopub.status.busy": "2022-11-17T05:24:33.621891Z",
     "iopub.status.idle": "2022-11-17T05:24:33.635074Z",
     "shell.execute_reply": "2022-11-17T05:24:33.633971Z",
     "shell.execute_reply.started": "2022-11-17T05:24:33.622514Z"
    }
   },
   "outputs": [],
   "source": [
    "# file information\n",
    "directory = '../input/open-problems-multimodal/'\n",
    "xfilename = 'train_multi_inputs.h5'\n",
    "yfilename = 'train_multi_targets.h5'\n",
    "metafilename = 'metadata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:33.636956Z",
     "iopub.status.busy": "2022-11-17T05:24:33.636621Z",
     "iopub.status.idle": "2022-11-17T05:24:34.305364Z",
     "shell.execute_reply": "2022-11-17T05:24:34.304068Z",
     "shell.execute_reply.started": "2022-11-17T05:24:33.636926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105942, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>day</th>\n",
       "      <th>donor</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>technology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56390cf1b95e</td>\n",
       "      <td>2</td>\n",
       "      <td>32606</td>\n",
       "      <td>NeuP</td>\n",
       "      <td>multiome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fc0c60183c33</td>\n",
       "      <td>2</td>\n",
       "      <td>32606</td>\n",
       "      <td>HSC</td>\n",
       "      <td>multiome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9b4a87e22ad0</td>\n",
       "      <td>2</td>\n",
       "      <td>32606</td>\n",
       "      <td>MasP</td>\n",
       "      <td>multiome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81cccad8cd81</td>\n",
       "      <td>2</td>\n",
       "      <td>32606</td>\n",
       "      <td>HSC</td>\n",
       "      <td>multiome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15cb3d85c232</td>\n",
       "      <td>2</td>\n",
       "      <td>32606</td>\n",
       "      <td>MkP</td>\n",
       "      <td>multiome</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cell_id  day  donor cell_type technology\n",
       "0  56390cf1b95e    2  32606      NeuP   multiome\n",
       "1  fc0c60183c33    2  32606       HSC   multiome\n",
       "2  9b4a87e22ad0    2  32606      MasP   multiome\n",
       "3  81cccad8cd81    2  32606       HSC   multiome\n",
       "4  15cb3d85c232    2  32606       MkP   multiome"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get metadata for samples\n",
    "metadf = getmeta(directory, metafilename, xfilename)\n",
    "print(metadf.shape)\n",
    "metadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:34.310708Z",
     "iopub.status.busy": "2022-11-17T05:24:34.310218Z",
     "iopub.status.idle": "2022-11-17T05:24:34.342451Z",
     "shell.execute_reply": "2022-11-17T05:24:34.341363Z",
     "shell.execute_reply.started": "2022-11-17T05:24:34.310674Z"
    }
   },
   "outputs": [],
   "source": [
    "# private test set will be from day 10, day 10 data not available for training\n",
    "# to simulate test situation...\n",
    "# indexes of day 7 samples for validation, days 2, 3, 4 for training\n",
    "day7 = list(metadf[metadf.day == 7].index)\n",
    "notday7 = list(metadf[~(metadf.day == 7)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:34.344317Z",
     "iopub.status.busy": "2022-11-17T05:24:34.343893Z",
     "iopub.status.idle": "2022-11-17T05:24:34.348956Z",
     "shell.execute_reply": "2022-11-17T05:24:34.348128Z",
     "shell.execute_reply.started": "2022-11-17T05:24:34.344276Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # find max values of input and output for scaling data\n",
    "# trainxmax, trainymax = getmax(directory, xfilename, yfilename, indexes=notday7)\n",
    "# xmax, ymax = getmax(directory, xfilename, yfilename, indexes=None)\n",
    "\n",
    "# # print(trainxmax, trainymax)\n",
    "# # print(xmax, ymax)\n",
    "# np.save('trainxmax', trainxmax)\n",
    "# np.save('trainymax', trainymax)\n",
    "# np.save('xmax', xmax)\n",
    "# np.save('ymax', ymax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:34.351240Z",
     "iopub.status.busy": "2022-11-17T05:24:34.350531Z",
     "iopub.status.idle": "2022-11-17T05:24:34.363601Z",
     "shell.execute_reply": "2022-11-17T05:24:34.362426Z",
     "shell.execute_reply.started": "2022-11-17T05:24:34.351206Z"
    }
   },
   "outputs": [],
   "source": [
    "trainxmax = np.load('trainxmax.npy')\n",
    "trainymax = np.load('trainymax.npy')\n",
    "xmax = np.load('xmax.npy')\n",
    "ymax = np.load('ymax.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:34.377006Z",
     "iopub.status.busy": "2022-11-17T05:24:34.376622Z",
     "iopub.status.idle": "2022-11-17T05:24:34.388888Z",
     "shell.execute_reply": "2022-11-17T05:24:34.387592Z",
     "shell.execute_reply.started": "2022-11-17T05:24:34.376975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# make dataset\n",
    "dataset = MakeDataset(directory=directory, \n",
    "                      xfile=xfilename, \n",
    "                      yfile=yfilename,\n",
    "                      metadata=metadf,\n",
    "                      xmax=trainxmax,\n",
    "                      ymax=trainymax, \n",
    "                      indexes=notday7)\n",
    "\n",
    "# private test set will be from day 10, day 10 data not available for training\n",
    "# to simulate test situation...\n",
    "# get indexes of day 7 samples for validation, days 2, 3, 4 for training\n",
    "print(len(dataset) == (len(notday7) + len(day7)))\n",
    "\n",
    "# split dataset into train and validation subsets based on day of sample\n",
    "trainset = torch.utils.data.Subset(dataset, notday7)\n",
    "valset = torch.utils.data.Subset(dataset, day7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:34.390956Z",
     "iopub.status.busy": "2022-11-17T05:24:34.390402Z",
     "iopub.status.idle": "2022-11-17T05:24:34.467649Z",
     "shell.execute_reply": "2022-11-17T05:24:34.466809Z",
     "shell.execute_reply.started": "2022-11-17T05:24:34.390923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.5000])\n",
      "tensor([0.0000, 0.7853, 0.0000,  ..., 0.0000, 0.0000, 0.7100])\n"
     ]
    }
   ],
   "source": [
    "a,b = dataset[123]\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:34.470030Z",
     "iopub.status.busy": "2022-11-17T05:24:34.469060Z",
     "iopub.status.idle": "2022-11-17T05:24:34.533942Z",
     "shell.execute_reply": "2022-11-17T05:24:34.532774Z",
     "shell.execute_reply.started": "2022-11-17T05:24:34.469995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228943 --> 23418\n"
     ]
    }
   ],
   "source": [
    "# get input and output sizes\n",
    "x, y = dataset[0]\n",
    "\n",
    "inputsize = len(x)\n",
    "outputsize = len(y)\n",
    "\n",
    "print(f'{inputsize} --> {outputsize}')\n",
    "\n",
    "# hyperparameters\n",
    "batchsize = 256\n",
    "hiddensize = 128\n",
    "\n",
    "epochs = 3\n",
    "learnrate = 0.001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:34.536122Z",
     "iopub.status.busy": "2022-11-17T05:24:34.535702Z",
     "iopub.status.idle": "2022-11-17T05:24:34.544644Z",
     "shell.execute_reply": "2022-11-17T05:24:34.543708Z",
     "shell.execute_reply.started": "2022-11-17T05:24:34.536082Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "train_dataloader = DataLoader(trainset, batch_size=batchsize, shuffle=True)\n",
    "val_dataloader = DataLoader(valset, batch_size=batchsize, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:34.547189Z",
     "iopub.status.busy": "2022-11-17T05:24:34.546105Z",
     "iopub.status.idle": "2022-11-17T05:24:34.553885Z",
     "shell.execute_reply": "2022-11-17T05:24:34.553042Z",
     "shell.execute_reply.started": "2022-11-17T05:24:34.547155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "# get gpu if available, otherwise use cpu\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'using {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:34.556143Z",
     "iopub.status.busy": "2022-11-17T05:24:34.555205Z",
     "iopub.status.idle": "2022-11-17T05:24:34.565212Z",
     "shell.execute_reply": "2022-11-17T05:24:34.564127Z",
     "shell.execute_reply.started": "2022-11-17T05:24:34.556108Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "class makenn(nn.Module):\n",
    "    def __init__(self, inputnodes, outputnodes, hidnodes):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(inputnodes, hidnodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidnodes, hidnodes),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidnodes, outputnodes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:34.567577Z",
     "iopub.status.busy": "2022-11-17T05:24:34.566456Z",
     "iopub.status.idle": "2022-11-17T05:24:34.898847Z",
     "shell.execute_reply": "2022-11-17T05:24:34.898007Z",
     "shell.execute_reply.started": "2022-11-17T05:24:34.567522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "makenn(\n",
      "  (layer_stack): Sequential(\n",
      "    (0): Linear(in_features=228943, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=128, out_features=23418, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initialize network\n",
    "model = makenn(inputsize, outputsize, hiddensize).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:34.900921Z",
     "iopub.status.busy": "2022-11-17T05:24:34.900093Z",
     "iopub.status.idle": "2022-11-17T05:24:34.905627Z",
     "shell.execute_reply": "2022-11-17T05:24:34.904619Z",
     "shell.execute_reply.started": "2022-11-17T05:24:34.900887Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss function and optimizer\n",
    "lossfunc = F.mse_loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learnrate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T05:24:34.907664Z",
     "iopub.status.busy": "2022-11-17T05:24:34.907269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train batch 0\n",
      "tensor(0.0900, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "\n",
    "# X, y = next(iter(train_dataloader))\n",
    "trainlosses = []\n",
    "vallosses = []\n",
    "# def train(dataloader, model, loss_fn, optimizer):\n",
    "for epoch in range(epochs):\n",
    "#     print(f'epoch {epoch}')\n",
    "    te_losses = []\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        # send to device\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # put in train mode\n",
    "        model.train()\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        tloss = lossfunc(pred, y)\n",
    "        # store loss values\n",
    "        te_losses.append(tloss.item())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        tloss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print(f'epoch {epoch}, train batch {batch}')\n",
    "            print(tloss)\n",
    "            \n",
    "    trainlosses.append(sum(te_losses)/len(te_losses))\n",
    "    \n",
    "    ve_losses = []\n",
    "    for batch, (valX, valy) in enumerate(val_dataloader):\n",
    "        \n",
    "        # put in evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            # send to device\n",
    "            valX = valX.to(device)\n",
    "            valy = valy.to(device)\n",
    "\n",
    "            pred = model(valX)\n",
    "            vloss = lossfunc(pred, valy)\n",
    "            vallosses.append(vloss.item())\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print(f'epoch {epoch}, val batch {batch}')\n",
    "            print(vloss)\n",
    "            \n",
    "    vallosses.append(sum(ve_losses)/len(ve_losses))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "print(len(trainlosses))\n",
    "plt.plot(np.arange(len(trainlosses)), trainlosses, label='train')\n",
    "plt.plot(np.arange(len(vallosses)), vallosses, label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
